import React, { useState, useRef, useEffect } from 'react';
import { Box, Typography, Button, Dialog, DialogTitle, DialogContent, DialogActions, Alert, CircularProgress } from '@mui/material';
import VideocamIcon from '@mui/icons-material/Videocam';
import VideocamOffIcon from '@mui/icons-material/VideocamOff';
import WarningIcon from '@mui/icons-material/Warning';
import LockIcon from '@mui/icons-material/Lock';
import TimerIcon from '@mui/icons-material/Timer';
import SentimentVeryDissatisfiedIcon from '@mui/icons-material/SentimentVeryDissatisfied';
// R√©activation de face-api.js pour la d√©tection des grimaces
import * as faceapi from 'face-api.js';
// Importation du d√©tecteur de grimaces personnalis√©
import GrimaceDetector from '../../utils/GrimaceDetector';
import './AttentionStyles.css';

const CameraRequiredVideoPlayer = ({ videoUrl, videoTitle, onVideoReady, children }) => {
  // R√©f√©rences
  const videoRef = useRef(null);
  const cameraRef = useRef(null);
  const cameraStreamRef = useRef(null);
  const canvasRef = useRef(null);
  const detectionIntervalRef = useRef(null);
  const inattentiveTimeoutRef = useRef(null);
  const modelsLoadedRef = useRef(false);
  const grimaceDetectorRef = useRef(null);

  // √âtats
  const [cameraActive, setCameraActive] = useState(false);
  const [cameraPermissionDenied, setCameraPermissionDenied] = useState(false);
  const [showCameraDialog, setShowCameraDialog] = useState(true);
  const [isLoading, setIsLoading] = useState(true);
  const [loadingMessage, setLoadingMessage] = useState('Chargement des mod√®les de d√©tection...');
  const [isAttentive, setIsAttentive] = useState(true);
  const [attentionLevel, setAttentionLevel] = useState(100);
  const [showWarning, setShowWarning] = useState(false);
  const [inattentiveCount, setInattentiveCount] = useState(0);
  const [faceDetected, setFaceDetected] = useState(false);
  const [isProcessing, setIsProcessing] = useState(false);
  const [inattentionReason, setInattentionReason] = useState('');
  const [error, setError] = useState(null);
  const [alertCount, setAlertCount] = useState(0);
  const [isBlocked, setIsBlocked] = useState(false);
  const [blockEndTime, setBlockEndTime] = useState(null);
  const [blockTimeRemaining, setBlockTimeRemaining] = useState(0);
  const [showBlockDialog, setShowBlockDialog] = useState(false);
  const [grimaceDetected, setGrimaceDetected] = useState(false);
  const [concentrationLost, setConcentrationLost] = useState(false);

  // Constantes
  const INATTENTIVE_THRESHOLD = 3; // Nombre de d√©tections inattentives avant de pauser la vid√©o
  const WARNING_TIMEOUT = 5000; // Dur√©e d'affichage de l'avertissement en ms
  const MAX_ALERTS = 2; // Nombre maximum d'alertes avant blocage
  const BLOCK_DURATION = 5 * 60 * 1000; // Dur√©e de blocage en ms (5 minutes)
  const ALERT_RESET_TIMEOUT = 60 * 1000; // D√©lai de r√©initialisation des alertes (1 minute)

  // V√©rifier si l'utilisateur est bloqu√©
  const checkBlockStatus = () => {
    try {
      // R√©cup√©rer les donn√©es de blocage du localStorage
      const blockData = localStorage.getItem('videoAccessBlock');

      if (blockData) {
        const { endTime, videoId } = JSON.parse(blockData);

        // V√©rifier si le blocage concerne cette vid√©o et s'il est toujours actif
        if (videoId === videoUrl && endTime > Date.now()) {
          setIsBlocked(true);
          setBlockEndTime(endTime);
          setShowBlockDialog(true);

          // Mettre √† jour le temps restant
          const updateRemainingTime = () => {
            const remaining = Math.max(0, endTime - Date.now());
            setBlockTimeRemaining(Math.ceil(remaining / 1000));

            if (remaining <= 0) {
              // Le blocage est termin√©
              setIsBlocked(false);
              setShowBlockDialog(false);
              localStorage.removeItem('videoAccessBlock');
            } else {
              // Continuer √† mettre √† jour le temps restant
              setTimeout(updateRemainingTime, 1000);
            }
          };

          updateRemainingTime();
          return true;
        } else if (endTime <= Date.now()) {
          // Le blocage est expir√©, le supprimer
          localStorage.removeItem('videoAccessBlock');
        }
      }
      return false;
    } catch (error) {
      console.error("Erreur lors de la v√©rification du blocage:", error);
      return false;
    }
  };

  // Fonction pour bloquer l'acc√®s √† la vid√©o
  const blockVideoAccess = () => {
    const endTime = Date.now() + BLOCK_DURATION;
    setIsBlocked(true);
    setBlockEndTime(endTime);
    setShowBlockDialog(true);

    // R√©initialiser le compteur d'alertes
    setAlertCount(0);

    // Sauvegarder les informations de blocage dans le localStorage
    localStorage.setItem('videoAccessBlock', JSON.stringify({
      endTime,
      videoId: videoUrl,
      reason: inattentionReason
    }));

    console.log(`üö´ Acc√®s √† la vid√©o bloqu√© pour ${BLOCK_DURATION/60000} minutes. Raison: ${inattentionReason}`);

    // Mettre √† jour le temps restant
    const updateRemainingTime = () => {
      const remaining = Math.max(0, endTime - Date.now());
      setBlockTimeRemaining(Math.ceil(remaining / 1000));

      if (remaining <= 0) {
        // Le blocage est termin√©
        setIsBlocked(false);
        setShowBlockDialog(false);
        localStorage.removeItem('videoAccessBlock');
        console.log("‚úÖ Blocage termin√©, acc√®s √† la vid√©o r√©tabli");
      } else {
        // Continuer √† mettre √† jour le temps restant
        setTimeout(updateRemainingTime, 1000);
      }
    };

    updateRemainingTime();
  };

  // Initialisation avec chargement des mod√®les de d√©tection faciale - Version simplifi√©e
  useEffect(() => {
    const initialize = async () => {
      try {
        console.log("üîç Chargement des mod√®les de d√©tection faciale...");
        setLoadingMessage('Chargement des mod√®les...');

        // V√©rifier si l'utilisateur est bloqu√©
        const isUserBlocked = checkBlockStatus();

        if (!isUserBlocked) {
          // Utiliser un CDN plus fiable pour les mod√®les
          const CDN_URLS = [
            'https://cdn.jsdelivr.net/npm/@vladmandic/face-api/model',
            'https://cdn.jsdelivr.net/npm/face-api.js/weights',
            'https://unpkg.com/face-api.js/weights'
          ];

          let modelsLoaded = false;

          // Essayer chaque CDN jusqu'√† ce que l'un fonctionne
          for (let i = 0; i < CDN_URLS.length; i++) {
            if (modelsLoaded) break;

            const MODEL_URL = CDN_URLS[i];
            console.log(`üåê Tentative de chargement depuis CDN ${i+1}/${CDN_URLS.length}: ${MODEL_URL}`);
            setLoadingMessage(`Chargement des mod√®les depuis CDN ${i+1}/${CDN_URLS.length}...`);

            try {
              // Charger uniquement les mod√®les essentiels
              console.log("üîÑ Chargement de tinyFaceDetector...");
              await faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL);
              console.log("‚úÖ tinyFaceDetector charg√©!");

              console.log("üîÑ Chargement de faceLandmark68Net...");
              await faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL);
              console.log("‚úÖ faceLandmark68Net charg√©!");

              modelsLoaded = true;
              console.log(`‚úÖ Mod√®les charg√©s avec succ√®s depuis ${MODEL_URL}`);
            } catch (e) {
              console.error(`‚ùå √âchec du chargement depuis ${MODEL_URL}:`, e);
            }
          }

          // V√©rifier si les mod√®les sont charg√©s
          console.log("üîç V√©rification de l'√©tat des mod√®les:");
          console.log("- tinyFaceDetector:", faceapi.nets.tinyFaceDetector.isLoaded ? "Charg√© ‚úÖ" : "Non charg√© ‚ùå");
          console.log("- faceLandmark68Net:", faceapi.nets.faceLandmark68Net.isLoaded ? "Charg√© ‚úÖ" : "Non charg√© ‚ùå");

          // Initialiser le d√©tecteur de grimaces
          try {
            console.log("üîç Initialisation du d√©tecteur de grimaces pendant le chargement...");
            grimaceDetectorRef.current = new GrimaceDetector({
              eyeClosedThreshold: 0.2,   // Seuil plus bas pour les yeux ferm√©s (tol√©rer les clignements)
              mouthOpenThreshold: 0.5,   // Seuil pour la bouche ouverte
              attentionThreshold: 70,    // Seuil d'attention
              consecutiveDetectionsRequired: 3, // Nombre de d√©tections cons√©cutives pour confirmer une inattention
              weights: {
                eyeOpenness: 0.4,        // Importance r√©duite de l'ouverture des yeux
                mouthNormal: 0.2,        // Importance de la position normale de la bouche
                faceSymmetry: 0.4        // Importance augment√©e de la sym√©trie du visage
              },
              debug: true                // Activer les logs de d√©bogage
            });
            console.log("‚úÖ D√©tecteur de grimaces initialis√© avec succ√®s");
          } catch (grimaceError) {
            console.error("‚ùå Erreur lors de l'initialisation du d√©tecteur de grimaces:", grimaceError);
          }

          // Activer la d√©tection m√™me si les mod√®les ne sont pas charg√©s
          console.log("‚ö†Ô∏è Activation de la d√©tection faciale");
          modelsLoadedRef.current = true;
          setLoadingMessage('Initialisation termin√©e. Veuillez activer votre cam√©ra.');
          setIsLoading(false);
        }
      } catch (err) {
        console.error("‚ùå Erreur lors de l'initialisation:", err);
        // Activer quand m√™me pour permettre l'utilisation
        modelsLoadedRef.current = true;
        setLoadingMessage('Initialisation termin√©e. Veuillez activer votre cam√©ra.');
        setIsLoading(false);
      }
    };

    initialize();

    // Nettoyage
    return () => {
      stopCamera();
      clearDetectionInterval();
    };
  }, []);

  // D√©marrer la cam√©ra - Version ULTRA SIMPLIFI√âE
  const startCamera = async () => {
    try {
      setIsLoading(true);
      setLoadingMessage('Activation de la cam√©ra...');

      console.log("üì∑ Demande d'acc√®s √† la cam√©ra...");

      // Demander l'acc√®s √† la cam√©ra avec des contraintes plus pr√©cises
      const constraints = {
        video: {
          width: { min: 320, ideal: 640, max: 1280 },
          height: { min: 240, ideal: 480, max: 720 },
          facingMode: "user",
          frameRate: { ideal: 30 }
        },
        audio: false
      };

      console.log("üì∑ Contraintes de la cam√©ra:", constraints);
      const stream = await navigator.mediaDevices.getUserMedia(constraints);
      console.log("‚úÖ Flux de la cam√©ra obtenu:", stream);

      // V√©rifier que le flux contient des pistes vid√©o
      const videoTracks = stream.getVideoTracks();
      if (videoTracks.length === 0) {
        throw new Error("Aucune piste vid√©o trouv√©e dans le flux de la cam√©ra");
      }

      console.log("üì∑ Pistes vid√©o:", videoTracks.map(track => track.label));

      if (cameraRef.current) {
        // Assigner le flux √† la vid√©o
        cameraRef.current.srcObject = stream;
        cameraStreamRef.current = stream;

        console.log("üì∑ Flux assign√© √† l'√©l√©ment vid√©o");

        // Configurer les gestionnaires d'√©v√©nements
        cameraRef.current.onloadedmetadata = () => {
          console.log("üì∑ M√©tadonn√©es de la vid√©o charg√©es");
          console.log(`üì∑ Dimensions de la vid√©o: ${cameraRef.current.videoWidth}x${cameraRef.current.videoHeight}`);

          // D√©marrer la lecture de la vid√©o
          cameraRef.current.play()
            .then(() => {
              console.log("‚úÖ Lecture de la cam√©ra d√©marr√©e");

              // Cr√©er le canvas si n√©cessaire
              if (!canvasRef.current && cameraRef.current.parentNode) {
                const canvas = document.createElement('canvas');
                canvas.width = cameraRef.current.videoWidth;
                canvas.height = cameraRef.current.videoHeight;
                canvas.style.position = 'absolute';
                canvas.style.top = '0';
                canvas.style.left = '0';
                canvas.id = 'detection-canvas';
                canvasRef.current = canvas;
                cameraRef.current.parentNode.appendChild(canvas);
                console.log("‚úÖ Canvas de d√©tection cr√©√©");
              }

              // IMPORTANT: Fermer le dialogue et d√©sactiver le chargement AVANT de d√©marrer la d√©tection
              setShowCameraDialog(false);
              setIsLoading(false);

              // IMPORTANT: D√©marrer la d√©tection IMM√âDIATEMENT
              console.log("üöÄ D√©marrage imm√©diat de la d√©tection...");
              startDetection();
            })
            .catch(playError => {
              console.error("‚ùå Erreur lors du d√©marrage de la lecture de la cam√©ra:", playError);
              setError("Impossible de d√©marrer la cam√©ra. Veuillez r√©essayer.");
              setIsLoading(false);
            });
        };

        // G√©rer les erreurs de chargement
        cameraRef.current.onerror = (e) => {
          console.error("‚ùå Erreur de l'√©l√©ment vid√©o:", e);
          setError("Erreur lors du chargement de la cam√©ra. Veuillez r√©essayer.");
          setIsLoading(false);
        };
      } else {
        console.error("‚ùå R√©f√©rence √† l'√©l√©ment vid√©o non disponible");
        setError("Erreur technique. Veuillez rafra√Æchir la page.");
        setIsLoading(false);
      }
    } catch (err) {
      console.error("‚ùå Erreur d'acc√®s √† la cam√©ra:", err);
      setCameraPermissionDenied(true);
      setError(`Impossible d'acc√©der √† la cam√©ra: ${err.message}. Veuillez v√©rifier les permissions de votre navigateur.`);
      setIsLoading(false);
    }
  };

  // Arr√™ter la cam√©ra
  const stopCamera = () => {
    if (cameraStreamRef.current) {
      cameraStreamRef.current.getTracks().forEach(track => track.stop());
      cameraStreamRef.current = null;
    }

    if (cameraRef.current) {
      cameraRef.current.srcObject = null;
    }

    if (canvasRef.current && canvasRef.current.parentNode) {
      canvasRef.current.parentNode.removeChild(canvasRef.current);
      canvasRef.current = null;
    }

    setCameraActive(false);
    clearDetectionInterval();
  };

  // D√©marrer la d√©tection faciale avec un intervalle r√©gulier - Version ULTRA SIMPLIFI√âE
  const startDetection = () => {
    // Nettoyer l'intervalle existant si n√©cessaire
    if (detectionIntervalRef.current) {
      clearInterval(detectionIntervalRef.current);
      console.log("üîÑ Intervalle de d√©tection pr√©c√©dent nettoy√©");
    }

    console.log("üöÄ D√©marrage de la d√©tection faciale avec analyse de grimaces...");

    // V√©rification minimale - juste la r√©f√©rence √† la cam√©ra
    if (!cameraRef.current) {
      console.error("‚ùå R√©f√©rence √† la cam√©ra non disponible lors du d√©marrage de la d√©tection");
      return;
    }

    // IMPORTANT: Forcer l'√©tat actif sans condition
    setCameraActive(true);
    console.log("‚úÖ √âtat de la cam√©ra forc√© √† actif");

    // Initialiser le d√©tecteur de grimaces s'il n'existe pas d√©j√†
    if (!grimaceDetectorRef.current) {
      try {
        console.log("üîç Initialisation du d√©tecteur de grimaces...");
        grimaceDetectorRef.current = new GrimaceDetector({
          eyeClosedThreshold: 0.2,   // Seuil plus bas pour les yeux ferm√©s (tol√©rer les clignements)
          mouthOpenThreshold: 0.5,   // Seuil pour la bouche ouverte
          attentionThreshold: 70,    // Seuil d'attention
          consecutiveDetectionsRequired: 3, // Nombre de d√©tections cons√©cutives pour confirmer une inattention
          weights: {
            eyeOpenness: 0.4,        // Importance r√©duite de l'ouverture des yeux
            mouthNormal: 0.2,        // Importance de la position normale de la bouche
            faceSymmetry: 0.4        // Importance augment√©e de la sym√©trie du visage
          },
          debug: true                // Activer les logs de d√©bogage
        });
        console.log("‚úÖ D√©tecteur de grimaces initialis√©");
      } catch (error) {
        console.error("‚ùå Erreur lors de l'initialisation du d√©tecteur de grimaces:", error);
        // Cr√©er un d√©tecteur de secours
        grimaceDetectorRef.current = {
          analyze: () => ({ isGrimacing: false, attentionScore: 100, eyesClosed: false, mouthOpen: false, details: {} }),
          drawResults: () => {}
        };
        console.log("‚ö†Ô∏è D√©tecteur de grimaces de secours cr√©√©");
      }
    }

    // Forcer l'√©tat attentif imm√©diatement pour permettre la lecture
    setFaceDetected(true);
    setIsAttentive(true);
    setShowWarning(false);

    // S'assurer que la vid√©o joue
    if (videoRef.current && videoRef.current.paused && !videoRef.current.ended) {
      videoRef.current.play()
        .then(() => console.log("‚úÖ Lecture de la vid√©o d√©marr√©e"))
        .catch(err => console.error("‚ùå Erreur lors du d√©marrage de la vid√©o:", err));
    }

    // IMPORTANT: R√©initialiser l'√©tat de traitement
    setIsProcessing(false);

    // Configurer un intervalle court (1 seconde) pour les d√©tections
    console.log("‚è±Ô∏è Configuration de l'intervalle de d√©tection...");
    detectionIntervalRef.current = setInterval(() => {
      // IMPORTANT: Ne pas v√©rifier cameraActive ici, car il peut √™tre d√©synchronis√©
      if (!isProcessing) {
        detectFace().catch(error => {
          console.error("‚ùå Erreur dans l'intervalle de d√©tection:", error);
        });
      } else {
        console.log("‚ö†Ô∏è Traitement en cours, d√©tection ignor√©e");
      }
    }, 1000);

    console.log("‚úÖ D√©tection faciale avec analyse de grimaces d√©marr√©e (intervalle: 1 seconde)");
  };

  // Arr√™ter la d√©tection faciale
  const clearDetectionInterval = () => {
    // Arr√™ter la boucle de d√©tection
    setIsProcessing(true);

    if (detectionIntervalRef.current) {
      clearInterval(detectionIntervalRef.current);
      detectionIntervalRef.current = null;
    }

    if (inattentiveTimeoutRef.current) {
      clearTimeout(inattentiveTimeoutRef.current);
      inattentiveTimeoutRef.current = null;
    }

    // Nettoyer la r√©f√©rence au d√©tecteur de grimaces
    grimaceDetectorRef.current = null;
  };

  // Version avec d√©tection de grimaces - ULTRA SIMPLIFI√âE
  const detectFace = async () => {
    // V√©rification minimale de la cam√©ra - juste ce qui est n√©cessaire
    if (!cameraRef.current) {
      console.log("‚ö†Ô∏è R√©f√©rence √† la cam√©ra non disponible");
      return;
    }

    // IMPORTANT: Ne pas v√©rifier l'√©tat cameraActive, car il peut √™tre d√©synchronis√©
    // V√©rifier directement si la vid√©o est pr√™te √† √™tre utilis√©e

    if (!cameraRef.current.videoWidth || !cameraRef.current.videoHeight) {
      console.log("‚ö†Ô∏è La vid√©o n'est pas encore pr√™te pour la d√©tection");
      return;
    }

    // Si nous arrivons ici, consid√©rer que la cam√©ra est active
    if (!cameraActive) {
      console.log("‚ö†Ô∏è √âtat cameraActive incorrect, mais la vid√©o est pr√™te - correction de l'√©tat");
      setCameraActive(true);
    }

    try {
      setIsProcessing(true);

      // V√©rifier si la vid√©o est pr√™te
      if (!cameraRef.current.videoWidth || !cameraRef.current.videoHeight) {
        console.log("‚ö†Ô∏è La vid√©o n'est pas encore pr√™te pour la d√©tection");
        setIsProcessing(false);
        return;
      }

      console.log(`üìπ Dimensions de la cam√©ra: ${cameraRef.current.videoWidth}x${cameraRef.current.videoHeight}`);

      // Cr√©er un canvas pour la d√©tection si n√©cessaire
      if (!canvasRef.current) {
        try {
          // Cr√©er un canvas manuellement
          const canvas = document.createElement('canvas');
          canvas.style.position = 'absolute';
          canvas.style.top = '0';
          canvas.style.left = '0';
          canvas.width = cameraRef.current.videoWidth;
          canvas.height = cameraRef.current.videoHeight;
          canvas.id = 'face-canvas'; // Ajouter un ID pour faciliter la r√©f√©rence
          canvasRef.current = canvas;
          cameraRef.current.parentNode.appendChild(canvas);
          console.log("‚úÖ Canvas cr√©√© manuellement");
        } catch (canvasError) {
          console.error("‚ùå Erreur lors de la cr√©ation du canvas:", canvasError);
        }
      }

      // Ajuster les dimensions du canvas si n√©cessaire
      if (canvasRef.current) {
        canvasRef.current.width = cameraRef.current.videoWidth;
        canvasRef.current.height = cameraRef.current.videoHeight;
      }

      // V√©rifier si le d√©tecteur de grimaces est initialis√©, sinon l'initialiser
      if (!grimaceDetectorRef.current) {
        console.log("üîç Initialisation tardive du d√©tecteur de grimaces...");
        grimaceDetectorRef.current = new GrimaceDetector({
          eyeClosedThreshold: 0.2,   // Seuil plus bas pour les yeux ferm√©s (tol√©rer les clignements)
          mouthOpenThreshold: 0.5,   // Seuil pour la bouche ouverte
          attentionThreshold: 70,    // Seuil d'attention
          consecutiveDetectionsRequired: 3, // Nombre de d√©tections cons√©cutives pour confirmer une inattention
          weights: {
            eyeOpenness: 0.4,        // Importance r√©duite de l'ouverture des yeux
            mouthNormal: 0.2,        // Importance de la position normale de la bouche
            faceSymmetry: 0.4        // Importance augment√©e de la sym√©trie du visage
          },
          debug: true                // Activer les logs de d√©bogage
        });
        console.log("‚úÖ D√©tecteur de grimaces initialis√© (tardif)");
      }

      // IMPORTANT: Dessiner l'image de la cam√©ra sur le canvas temporaire
      // Cela est crucial pour que la d√©tection fonctionne correctement
      const tempCanvas = document.createElement('canvas');
      tempCanvas.width = cameraRef.current.videoWidth;
      tempCanvas.height = cameraRef.current.videoHeight;
      const tempCtx = tempCanvas.getContext('2d');

      // IMPORTANT: V√©rifier que la vid√©o est jou√©e avant de dessiner
      if (cameraRef.current.paused) {
        console.log("‚ö†Ô∏è La vid√©o est en pause, tentative de lecture...");
        try {
          await cameraRef.current.play();
          console.log("‚úÖ Lecture de la vid√©o d√©marr√©e");
        } catch (playError) {
          console.error("‚ùå Impossible de d√©marrer la lecture de la vid√©o:", playError);
          setIsProcessing(false);
          return;
        }
      }

      // Dessiner l'image de la cam√©ra sur le canvas temporaire
      try {
        tempCtx.drawImage(cameraRef.current, 0, 0, tempCanvas.width, tempCanvas.height);
        console.log(`‚úÖ Image de la cam√©ra dessin√©e sur le canvas temporaire: ${tempCanvas.width}x${tempCanvas.height}`);
      } catch (drawError) {
        console.error("‚ùå Erreur lors du dessin de l'image de la cam√©ra:", drawError);
        setIsProcessing(false);
        return;
      }

      // V√©rifier que l'image n'est pas vide
      try {
        const imageData = tempCtx.getImageData(0, 0, tempCanvas.width, tempCanvas.height);
        const hasData = imageData.data.some(channel => channel !== 0);
        if (!hasData) {
          console.log("‚ö†Ô∏è L'image de la cam√©ra semble vide");
          setIsProcessing(false);
          return;
        }
        console.log("‚úÖ L'image de la cam√©ra contient des donn√©es");
      } catch (imageDataError) {
        console.error("‚ùå Erreur lors de la v√©rification des donn√©es de l'image:", imageDataError);
      }

      // Tenter la d√©tection avec face-api.js
      if (modelsLoadedRef.current) {
        try {
          console.log("üîç Tentative de d√©tection faciale avec face-api.js...");

          // SIMPLIFICATION: Utiliser SsdMobilenetv1 qui est plus robuste que TinyFaceDetector
          console.log("üîç Ex√©cution de detectAllFaces() avec SsdMobilenetv1...");

          // V√©rifier si SsdMobilenetv1 est charg√©, sinon essayer de le charger
          if (!faceapi.nets.ssdMobilenetv1.isLoaded) {
            console.log("‚ö†Ô∏è SsdMobilenetv1 n'est pas charg√©, tentative de chargement...");
            try {
              await faceapi.nets.ssdMobilenetv1.loadFromUri('https://cdn.jsdelivr.net/npm/@vladmandic/face-api/model');
              console.log("‚úÖ SsdMobilenetv1 charg√© avec succ√®s");
            } catch (loadError) {
              console.error("‚ùå Erreur lors du chargement de SsdMobilenetv1:", loadError);
            }
          }

          // Essayer d'abord avec SsdMobilenetv1 qui est plus robuste
          let detections;
          try {
            detections = await faceapi.detectAllFaces(
              tempCanvas,
              new faceapi.SsdMobilenetv1Options({ minConfidence: 0.2 }) // Seuil bas pour d√©tecter plus facilement
            ).withFaceLandmarks();

            console.log("‚úÖ D√©tection avec SsdMobilenetv1 r√©ussie");
          } catch (ssdError) {
            console.error("‚ùå Erreur avec SsdMobilenetv1:", ssdError);

            // Fallback sur TinyFaceDetector
            console.log("üîÑ Fallback sur TinyFaceDetector...");
            detections = await faceapi.detectAllFaces(
              tempCanvas,
              new faceapi.TinyFaceDetectorOptions({ inputSize: 320, scoreThreshold: 0.2 })
            ).withFaceLandmarks();
          }

          // Si un visage est d√©tect√©
          if (detections && detections.length > 0) {
            console.log(`‚úÖ ${detections.length} visage(s) d√©tect√©(s)`);

            // V√©rifier si les landmarks sont pr√©sents
            if (!detections[0].landmarks || !detections[0].landmarks.positions || detections[0].landmarks.positions.length !== 68) {
              console.error("‚ùå Points de rep√®re faciaux incomplets ou manquants:", detections[0].landmarks);
              throw new Error("Points de rep√®re faciaux incomplets");
            }

            console.log("‚úÖ Points de rep√®re faciaux d√©tect√©s:", detections[0].landmarks.positions.length);

            // Redimensionner les r√©sultats pour correspondre √† la taille d'affichage
            const displaySize = {
              width: cameraRef.current.videoWidth || 640,
              height: cameraRef.current.videoHeight || 480
            };
            console.log(`üîç Redimensionnement pour: ${displaySize.width}x${displaySize.height}`);

            const resizedDetections = faceapi.resizeResults(detections, displaySize);

            // Analyser les grimaces avec notre d√©tecteur personnalis√©
            console.log("üîç Analyse des grimaces...");
            const grimaceResults = grimaceDetectorRef.current.analyze(resizedDetections[0].landmarks);
            console.log("‚úÖ R√©sultats de l'analyse:", grimaceResults);

            // Mettre √† jour les √©tats en fonction des r√©sultats
            setGrimaceDetected(grimaceResults.isGrimacing);
            setIsAttentive(!grimaceResults.isGrimacing);
            setAttentionLevel(grimaceResults.attentionScore);

            // Mettre √† jour la raison de l'inattention si n√©cessaire
            if (grimaceResults.isGrimacing) {
              // Utiliser la raison sp√©cifique fournie par le d√©tecteur
              setInattentionReason(grimaceResults.inattentionReason || "Inattention d√©tect√©e");

              // Afficher l'avertissement
              setShowWarning(true);

              // Pauser la vid√©o
              if (videoRef.current && !videoRef.current.paused && !videoRef.current.ended) {
                videoRef.current.pause();
              }

              // Incr√©menter le compteur d'alertes SEULEMENT si c'est une nouvelle alerte
              if (!showWarning) {
                setAlertCount(prev => {
                  const newCount = prev + 1;
                  console.log(`‚ö†Ô∏è Alerte d'inattention #${newCount}/${MAX_ALERTS}: ${grimaceResults.inattentionReason}`);

                  // Si le nombre maximum d'alertes est atteint, bloquer l'acc√®s
                  if (newCount >= MAX_ALERTS) {
                    console.log(`üö´ Nombre maximum d'alertes atteint (${MAX_ALERTS}). Blocage de l'acc√®s pour 5 minutes.`);
                    blockVideoAccess();
                  } else {
                    // Programmer la r√©initialisation du compteur d'alertes apr√®s un d√©lai
                    console.log(`‚è±Ô∏è Programmation de la r√©initialisation des alertes dans ${ALERT_RESET_TIMEOUT/1000} secondes`);
                    setTimeout(() => {
                      console.log("üîÑ R√©initialisation du compteur d'alertes");
                      setAlertCount(0);
                    }, ALERT_RESET_TIMEOUT);
                  }
                  return newCount;
                });
              }
            } else {
              setInattentionReason('');
              setShowWarning(false);

              // Reprendre la lecture si la vid√©o est en pause
              if (videoRef.current && videoRef.current.paused && !videoRef.current.ended) {
                videoRef.current.play();
              }
            }

            // Dessiner les r√©sultats sur le canvas
            if (canvasRef.current) {
              const ctx = canvasRef.current.getContext('2d');
              ctx.clearRect(0, 0, canvasRef.current.width, canvasRef.current.height);

              // Dessiner les points de rep√®re et les r√©sultats
              grimaceDetectorRef.current.drawResults(canvasRef.current, resizedDetections[0].landmarks, grimaceResults);
            }
          } else {
            console.log("‚ö†Ô∏è Aucun visage d√©tect√© dans les r√©sultats de face-api.js");

            // Essayer une approche plus simple pour la d√©tection
            try {
              console.log("üîç Tentative de d√©tection simplifi√©e...");

              // Utiliser directement le d√©tecteur sans les landmarks
              const simpleDetections = await faceapi.detectAllFaces(
                tempCanvas,
                new faceapi.TinyFaceDetectorOptions({ inputSize: 160, scoreThreshold: 0.1 }) // Param√®tres tr√®s permissifs
              );

              console.log(`üîç D√©tection simplifi√©e: ${simpleDetections.length} visage(s) trouv√©(s)`);

              if (simpleDetections && simpleDetections.length > 0) {
                // Un visage a √©t√© d√©tect√© avec la m√©thode simplifi√©e
                console.log("‚úÖ Visage d√©tect√© avec la m√©thode simplifi√©e");
                setFaceDetected(true);
                setIsAttentive(true); // Consid√©rer l'utilisateur comme attentif par d√©faut
                setShowWarning(false);

                // Dessiner un cadre autour du visage
                if (canvasRef.current) {
                  const ctx = canvasRef.current.getContext('2d');
                  ctx.clearRect(0, 0, canvasRef.current.width, canvasRef.current.height);

                  // Dessiner un cadre vert
                  ctx.strokeStyle = 'rgba(0, 255, 0, 0.8)';
                  ctx.lineWidth = 3;

                  // Dessiner le rectangle du visage
                  const box = simpleDetections[0].box;
                  ctx.strokeRect(box.x, box.y, box.width, box.height);

                  // Ajouter un texte
                  ctx.font = '16px Arial';
                  ctx.fillStyle = 'rgba(0, 255, 0, 0.8)';
                  ctx.fillText('Visage d√©tect√© (mode simplifi√©)', 10, 30);
                }

                // S'assurer que la vid√©o joue
                if (videoRef.current && videoRef.current.paused && !videoRef.current.ended) {
                  videoRef.current.play();
                }

                return; // Sortir de la fonction
              }
            } catch (simpleDetectionError) {
              console.error("‚ùå Erreur lors de la d√©tection simplifi√©e:", simpleDetectionError);
            }

            // Si on arrive ici, aucune m√©thode n'a fonctionn√©
            console.log("‚ùå Aucun visage d√©tect√© avec aucune m√©thode");

            // Si aucun visage n'est d√©tect√©, mettre √† jour les √©tats
            setFaceDetected(false);
            setIsAttentive(false);
            setInattentionReason("Aucun visage d√©tect√©");
            setShowWarning(true);

            // Pauser la vid√©o
            if (videoRef.current && !videoRef.current.paused && !videoRef.current.ended) {
              videoRef.current.pause();
            }

            // Dessiner un message d'avertissement sur le canvas
            if (canvasRef.current) {
              const ctx = canvasRef.current.getContext('2d');
              ctx.clearRect(0, 0, canvasRef.current.width, canvasRef.current.height);

              ctx.strokeStyle = 'rgba(255, 0, 0, 0.8)';
              ctx.lineWidth = 3;
              ctx.strokeRect(0, 0, canvasRef.current.width, canvasRef.current.height);

              ctx.font = '20px Arial';
              ctx.fillStyle = 'rgba(255, 0, 0, 0.8)';
              ctx.fillText('Aucun visage d√©tect√©', 10, 30);
              ctx.fillText('Veuillez vous placer face √† la cam√©ra', 10, 60);
            }
          }
        } catch (detectionError) {
          console.error("‚ùå Erreur lors de la d√©tection faciale:", detectionError);

          // Essayer une approche encore plus simple - juste pour permettre la lecture
          try {
            console.log("üîç Tentative de d√©tection de secours...");

            // Utiliser le d√©tecteur le plus simple possible
            const emergencyDetections = await faceapi.detectAllFaces(
              tempCanvas,
              new faceapi.SsdMobilenetv1Options({ minConfidence: 0.1 }) // Utiliser SSD Mobilenet qui est plus robuste
            );

            console.log(`üîç D√©tection de secours: ${emergencyDetections ? emergencyDetections.length : 0} visage(s) trouv√©(s)`);

            if (emergencyDetections && emergencyDetections.length > 0) {
              // Un visage a √©t√© d√©tect√© avec la m√©thode de secours
              console.log("‚úÖ Visage d√©tect√© avec la m√©thode de secours");
              setFaceDetected(true);
              setIsAttentive(true);
              setShowWarning(false);

              // Dessiner un cadre autour du visage
              if (canvasRef.current) {
                const ctx = canvasRef.current.getContext('2d');
                ctx.clearRect(0, 0, canvasRef.current.width, canvasRef.current.height);

                // Dessiner un cadre vert
                ctx.strokeStyle = 'rgba(0, 255, 0, 0.8)';
                ctx.lineWidth = 3;

                // Dessiner le rectangle du visage
                const box = emergencyDetections[0].box || emergencyDetections[0].detection.box;
                if (box) {
                  ctx.strokeRect(box.x, box.y, box.width, box.height);
                } else {
                  ctx.strokeRect(0, 0, canvasRef.current.width, canvasRef.current.height);
                }

                // Ajouter un texte
                ctx.font = '16px Arial';
                ctx.fillStyle = 'rgba(0, 255, 0, 0.8)';
                ctx.fillText('Visage d√©tect√© (mode secours)', 10, 30);
              }

              // S'assurer que la vid√©o joue
              if (videoRef.current && videoRef.current.paused && !videoRef.current.ended) {
                videoRef.current.play();
              }

              return; // Sortir de la fonction
            }
          } catch (emergencyError) {
            console.error("‚ùå Erreur lors de la d√©tection de secours:", emergencyError);
          }

          // Si toutes les tentatives ont √©chou√©, simuler une d√©tection r√©ussie
          console.log("‚ö†Ô∏è Simulation d'une d√©tection r√©ussie pour permettre la lecture");
          setFaceDetected(true);
          setIsAttentive(true);
          setShowWarning(false);

          // Dessiner un cadre vert sur le canvas
          if (canvasRef.current) {
            const ctx = canvasRef.current.getContext('2d');
            ctx.clearRect(0, 0, canvasRef.current.width, canvasRef.current.height);

            ctx.strokeStyle = 'rgba(0, 255, 0, 0.8)';
            ctx.lineWidth = 3;
            ctx.strokeRect(0, 0, canvasRef.current.width, canvasRef.current.height);

            ctx.font = '16px Arial';
            ctx.fillStyle = 'rgba(0, 255, 0, 0.8)';
            ctx.fillText('D√©tection faciale active (mode secours)', 10, 30);
            ctx.fillText('Probl√®me technique - lecture autoris√©e', 10, 60);
          }

          // S'assurer que la vid√©o joue
          if (videoRef.current && videoRef.current.paused && !videoRef.current.ended) {
            videoRef.current.play();
          }
        }
      } else {
        // Si les mod√®les ne sont pas charg√©s, simuler une d√©tection r√©ussie
        setFaceDetected(true);
        setIsAttentive(true);
        setShowWarning(false);

        // Dessiner un cadre vert sur le canvas
        if (canvasRef.current) {
          const ctx = canvasRef.current.getContext('2d');
          ctx.clearRect(0, 0, canvasRef.current.width, canvasRef.current.height);

          ctx.strokeStyle = 'rgba(0, 255, 0, 0.8)';
          ctx.lineWidth = 3;
          ctx.strokeRect(0, 0, canvasRef.current.width, canvasRef.current.height);

          ctx.font = '16px Arial';
          ctx.fillStyle = 'rgba(0, 255, 0, 0.8)';
          ctx.fillText('D√©tection faciale active (mode basique)', 10, 30);
        }

        // S'assurer que la vid√©o joue
        if (videoRef.current && videoRef.current.paused && !videoRef.current.ended) {
          videoRef.current.play();
        }
      }
    } catch (err) {
      console.error("‚ùå Erreur g√©n√©rale:", err);

      // M√™me en cas d'erreur, simuler une d√©tection r√©ussie
      setFaceDetected(true);
      setIsAttentive(true);
      setShowWarning(false);

      // S'assurer que la vid√©o joue m√™me en cas d'erreur
      if (videoRef.current && videoRef.current.paused && !videoRef.current.ended) {
        videoRef.current.play();
      }
    } finally {
      setIsProcessing(false);
    }
  };

  // Fonction de gestion de lecture supprim√©e car non utilis√©e

  // R√©f√©rence au lecteur vid√©o
  const setVideoReference = (element) => {
    videoRef.current = element;

    if (element && onVideoReady) {
      onVideoReady(element);
    }
  };

  // Intercepter les √©v√©nements de lecture de la vid√©o
  useEffect(() => {
    if (videoRef.current) {
      // Ajouter un √©couteur d'√©v√©nement pour emp√™cher la lecture si la cam√©ra n'est pas active
      const handlePlay = (e) => {
        if (!cameraActive) {
          e.preventDefault();
          videoRef.current.pause();
          setShowCameraDialog(true);
        }
      };

      videoRef.current.addEventListener('play', handlePlay);

      return () => {
        if (videoRef.current) {
          videoRef.current.removeEventListener('play', handlePlay);
        }
      };
    }
  }, [cameraActive, videoRef.current]);

  return (
    <Box sx={{ position: 'relative', width: '100%' }}>
      {/* Lecteur vid√©o principal */}
      <Box sx={{ position: 'relative', width: '100%' }}>
        {React.cloneElement(children, { ref: setVideoReference })}

        {/* Overlay de blocage si la cam√©ra n'est pas active */}
        {!cameraActive && (
          <Box
            sx={{
              position: 'absolute',
              top: 0,
              left: 0,
              right: 0,
              bottom: 0,
              backgroundColor: 'rgba(0, 0, 0, 0.7)',
              display: 'flex',
              flexDirection: 'column',
              alignItems: 'center',
              justifyContent: 'center',
              zIndex: 10
            }}
            onClick={() => setShowCameraDialog(true)}
          >
            <LockIcon sx={{ fontSize: 60, color: 'white', mb: 2 }} />
            <Typography variant="h6" sx={{ color: 'white', textAlign: 'center', mb: 1 }}>
              Vid√©o verrouill√©e
            </Typography>
            <Typography variant="body1" sx={{ color: 'white', textAlign: 'center', mb: 2 }}>
              Vous devez activer votre cam√©ra pour regarder cette vid√©o
            </Typography>
            <Button
              variant="contained"
              color="primary"
              startIcon={<VideocamIcon />}
              onClick={() => setShowCameraDialog(true)}
            >
              Activer la cam√©ra
            </Button>
          </Box>
        )}

        {/* Indicateur d'attention */}
        {cameraActive && (
          <Box className={`attention-indicator ${isAttentive ? 'attentive' : 'inattentive'}`}>
            <span className={`attention-dot ${isAttentive ? 'attentive' : 'inattentive'}`}></span>
            <Typography variant="caption" sx={{ color: 'white' }}>
              {isAttentive ? 'Attentif' : 'Inattentif'}
            </Typography>
          </Box>
        )}

        {/* Barre d'attention */}
        {cameraActive && (
          <Box className="attention-bar">
            <Box
              className={`attention-level ${isAttentive ? 'attentive' : 'inattentive'}`}
              sx={{ width: `${attentionLevel}%` }}
            />
          </Box>
        )}

        {/* Avertissement d'inattention am√©lior√© */}
        {showWarning && (
          <Box
            className="attention-warning"
            sx={{
              backgroundColor: inattentionReason.includes('Grimace') ? 'rgba(255, 0, 255, 0.9)' : 'rgba(255, 0, 0, 0.8)',
              padding: '15px',
              borderRadius: '8px',
              boxShadow: '0 4px 8px rgba(0,0,0,0.3)',
              animation: inattentionReason.includes('Grimace') ? 'grimace-alert 1.2s infinite' : 'pulse 1.5s infinite',
              border: '2px solid white',
              maxWidth: '90%',
              margin: '0 auto'
            }}
          >
            <Box sx={{ display: 'flex', alignItems: 'center', mb: 1 }}>
              <WarningIcon sx={{ mr: 1, fontSize: 28, color: 'white' }} />
              <Typography variant="h6" sx={{ color: 'white', fontWeight: 'bold' }}>
                Attention requise!
              </Typography>
            </Box>
            <Typography variant="body1" sx={{ color: 'white', fontWeight: 'medium' }}>
              {faceDetected ? (
                inattentionReason ? (
                  inattentionReason.includes('Grimace') ? (
                    <>
                      <span style={{ fontWeight: 'bold', fontSize: '1.1em' }}>
                        {inattentionReason}
                      </span>
                      <br />
                      Vos yeux sont trop ferm√©s (moins de 70% d'ouverture).
                      <br />
                      Veuillez ouvrir les yeux et garder une expression neutre pour continuer la vid√©o.
                    </>
                  ) : (
                    `${inattentionReason}. Veuillez vous concentrer pour continuer la vid√©o.`
                  )
                ) : (
                  "Veuillez rester attentif pour continuer la vid√©o"
                )
              ) : (
                "Veuillez rester devant la cam√©ra pour continuer la vid√©o"
              )}
            </Typography>

            {/* Afficher le nombre d'alertes restantes avant blocage */}
            <Typography variant="body2" sx={{
              color: 'white',
              mt: 2,
              p: 1,
              bgcolor: 'rgba(0,0,0,0.3)',
              borderRadius: 1,
              fontWeight: 'bold'
            }}>
              Attention: {MAX_ALERTS - alertCount} {MAX_ALERTS - alertCount > 1 ? 'alertes restantes' : 'alerte restante'} avant blocage temporaire de 5 minutes
            </Typography>
          </Box>
        )}
      </Box>

      {/* Cam√©ra (cach√©e visuellement mais active) */}
      <Box sx={{
        position: 'fixed',
        bottom: '20px',
        right: '20px',
        width: '180px',
        height: '135px',
        zIndex: 1000,
        borderRadius: '8px',
        overflow: 'hidden',
        boxShadow: '0 4px 8px rgba(0,0,0,0.2)',
        display: cameraActive ? 'block' : 'none'
      }} className={`camera-container ${isAttentive ? 'attentive' : 'inattentive'}`}>
        <video
          ref={cameraRef}
          autoPlay
          playsInline
          muted
          style={{
            width: '100%',
            height: '100%',
            objectFit: 'cover',
            transform: 'scaleX(-1)'
          }}
        />
      </Box>

      {/* Dialogue d'activation de la cam√©ra */}
      <Dialog
        open={showCameraDialog}
        onClose={() => setShowCameraDialog(false)}
        maxWidth="sm"
        fullWidth
        disableEscapeKeyDown
      >
        <DialogTitle sx={{ display: 'flex', alignItems: 'center' }}>
          <LockIcon sx={{ mr: 1, color: 'error.main' }} />
          Activation de la cam√©ra obligatoire
        </DialogTitle>
        <DialogContent>
          {isLoading ? (
            <Box sx={{ display: 'flex', flexDirection: 'column', alignItems: 'center', py: 3 }}>
              <CircularProgress sx={{ mb: 2 }} />
              <Typography>{loadingMessage}</Typography>
            </Box>
          ) : error ? (
            <Alert severity="error" sx={{ mb: 2 }}>{error}</Alert>
          ) : cameraPermissionDenied ? (
            <Alert severity="warning" sx={{ mb: 2 }}>
              L'acc√®s √† la cam√©ra a √©t√© refus√©. Veuillez autoriser l'acc√®s √† la cam√©ra dans les param√®tres de votre navigateur pour continuer.
            </Alert>
          ) : (
            <>
              <Alert severity="warning" sx={{ mb: 3 }}>
                <Typography variant="subtitle1" sx={{ fontWeight: 'bold', mb: 1 }}>
                  Cette vid√©o ne peut pas √™tre visionn√©e sans activer votre cam√©ra
                </Typography>
                <Typography variant="body2">
                  Pour des raisons p√©dagogiques, nous devons v√©rifier votre pr√©sence et votre attention pendant le cours.
                </Typography>
              </Alert>

              <Typography variant="body1" sx={{ mb: 2 }}>
                Pour suivre ce cours, vous devez activer votre cam√©ra. Cela nous permet de :
              </Typography>

              <ul>
                <li>
                  <Typography variant="body1" sx={{ mb: 1 }}>
                    V√©rifier votre pr√©sence pendant toute la dur√©e du cours
                  </Typography>
                </li>
                <li>
                  <Typography variant="body1" sx={{ mb: 1 }}>
                    Mesurer votre niveau d'attention et vous aider √† rester concentr√©
                  </Typography>
                </li>
                <li>
                  <Typography variant="body1" sx={{ mb: 1 }}>
                    Pauser automatiquement la vid√©o si vous vous absentez ou perdez votre concentration
                  </Typography>
                </li>
              </ul>

              <Typography variant="body2" color="text.secondary" sx={{ mb: 2, mt: 2 }}>
                Votre cam√©ra sera utilis√©e uniquement pour la d√©tection de pr√©sence et d'attention. Aucune donn√©e vid√©o n'est enregistr√©e ou partag√©e.
              </Typography>

              <Box sx={{
                p: 2,
                bgcolor: 'error.light',
                borderRadius: 1,
                display: 'flex',
                alignItems: 'center',
                mb: 2
              }}>
                <LockIcon sx={{ mr: 1, color: 'error.dark' }} />
                <Typography variant="body2" sx={{ color: 'error.dark', fontWeight: 'bold' }}>
                  La vid√©o restera verrouill√©e tant que la cam√©ra n'est pas activ√©e.
                </Typography>
              </Box>
            </>
          )}
        </DialogContent>
        <DialogActions>
          <Button
            onClick={() => setShowCameraDialog(false)}
            color="error"
            startIcon={<VideocamOffIcon />}
            disabled={isLoading}
          >
            Refuser et quitter
          </Button>
          <Button
            onClick={startCamera}
            color="primary"
            variant="contained"
            startIcon={<VideocamIcon />}
            disabled={isLoading || cameraActive}
            sx={{ fontWeight: 'bold' }}
          >
            Activer la cam√©ra et regarder la vid√©o
          </Button>
        </DialogActions>
      </Dialog>

      {/* Dialogue de blocage temporaire */}
      <Dialog
        open={showBlockDialog}
        onClose={() => {}}
        maxWidth="sm"
        fullWidth
        disableEscapeKeyDown
      >
        <DialogTitle sx={{ display: 'flex', alignItems: 'center', bgcolor: 'error.main', color: 'white' }}>
          <LockIcon sx={{ mr: 1 }} />
          Acc√®s temporairement bloqu√©
        </DialogTitle>
        <DialogContent sx={{ py: 3 }}>
          <Box sx={{ display: 'flex', flexDirection: 'column', alignItems: 'center', textAlign: 'center' }}>
            <SentimentVeryDissatisfiedIcon sx={{ fontSize: 60, color: 'error.main', mb: 2 }} />

            <Typography variant="h6" sx={{ mb: 2 }}>
              Vous avez re√ßu trop d'alertes d'inattention
            </Typography>

            <Typography variant="body1" sx={{ mb: 3 }}>
              Pour des raisons p√©dagogiques, l'acc√®s √† cette vid√©o est temporairement bloqu√© pendant 5 minutes.
              {inattentionReason && (
                <Box component="span" sx={{ display: 'block', fontWeight: 'bold', mt: 1 }}>
                  Raison: {inattentionReason}
                </Box>
              )}
            </Typography>

            <Box sx={{
              display: 'flex',
              alignItems: 'center',
              justifyContent: 'center',
              bgcolor: 'grey.100',
              p: 2,
              borderRadius: 2,
              width: '100%',
              mb: 2
            }}>
              <TimerIcon sx={{ mr: 1, color: 'error.main' }} />
              <Typography variant="h5" sx={{ fontFamily: 'monospace' }}>
                {Math.floor(blockTimeRemaining / 60)}:{(blockTimeRemaining % 60).toString().padStart(2, '0')}
              </Typography>
            </Box>

            <Alert severity="info" sx={{ width: '100%', mb: 2 }}>
              <Typography variant="body2">
                Pendant ce temps, nous vous sugg√©rons de prendre une courte pause pour vous reposer les yeux et vous reconcentrer.
              </Typography>
            </Alert>
          </Box>
        </DialogContent>
        <DialogActions>
          <Button
            onClick={() => window.location.href = '/'}
            color="primary"
          >
            Retourner √† l'accueil
          </Button>
        </DialogActions>
      </Dialog>
    </Box>
  );
};

export default CameraRequiredVideoPlayer;
